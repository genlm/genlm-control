{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/genlm_backend/llm/vllm.py:19: UserWarning: vLLM not installed. Run 'pip install vllm' to use the vLLM-based AsyncLM model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from genlm_control.experimental.subtoken import SubtokenPotential\n",
    "from genlm_control import EOT, EOS, BoolFSA, PromptedLLM\n",
    "from arsenal.maths import sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtoken potentials\n",
    "\n",
    "> Reinterpreting a potential defined on a coarser token alphabet $\\mathcal{A}$ in terms of the finer-grained alphabet $\\mathcal{B}$.\n",
    "\n",
    "Suppose we have a potential $\\Phi$ defined over a (finite) alphabet $\\mathcal{A}$ with end-of-sequence token $\\textsf{eos}$. Let $\\mathcal{B}$ be a finer-grained alphabet such that $\\mathcal{A} \\subseteq \\mathcal{B}^*$. We reinterpret $\\Phi$ as a potential $\\Psi_{\\bm{x}}$ over $\\mathcal{B}$ by projecting $\\Phi$ from tokens in $\\mathcal{A}$ to sequences of subtokens in $\\mathcal{B}^*$, conditioned on a token context $\\bm{x} \\in \\mathcal{A}^*$.\n",
    "\n",
    "The complete potential is defined as:\n",
    "\n",
    "$$\n",
    "\\psi_{\\bm{x}}(\\bm{s}) = \\begin{cases}\n",
    "\\phi(\\bm{s} \\mid \\bm{x}) &\\text{if }\\bm{s} \\in \\mathcal{B}^* \\cup \\{\\textsf{eos}\\}\\\\\n",
    "0  &\\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And the next-(sub)token potential is defined as:\n",
    "\n",
    "$$\n",
    "\\Psi_{\\bm{x}}(s \\mid \\bm{s}) = \\begin{cases}\n",
    "\\frac{\\overrightarrow{\\psi}_{\\bm{x}}(\\bm{s}s)}{\\overrightarrow{\\psi}_{\\bm{x}}(\\bm{s})} &\\text{ if } s \\in \\mathcal{B}\\\\\n",
    "\\frac{\\psi_{\\bm{x}}(\\bm{s})}{\\overrightarrow{\\psi}_{\\bm{x}}(\\bm{s})} &\\text{ if } s = \\textsf{eot}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where we use $\\textsf{eot}$ (”end of token”) as the distinguished end symbol. \n",
    "\n",
    "### Marginalized subtoken potential\n",
    "\n",
    "As is the case for token-level potentials, the **prefix potential** can be chosen by the user so long as it satisfies absolute continuity and consistency. The **optimal prefix potential** is the marginal of the complete potential (prove):\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\psi}_{\\bm{x}}(\\bm{s}) = \\sum_{\\bm{s}' \\in \\mathcal{B}^* \\cup \\{\\textsf{eos}\\} : \\bm{s}' \\succeq \\bm{s}} \\psi_{\\bm{x}}(\\bm{s}')\n",
    "$$\n",
    "\n",
    "The optimal prefix potential can be efficiently parameterized for all $\\bm{s} \\in \\mathcal{A} \\cup \\{\\textsf{eos}\\}$ using a single sparse matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/genlm_backend/tokenization/vocab.py:99: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = PromptedLLM.from_name(\"gpt2\", backend=\"hf\", temperature=0.5)\n",
    "llm.set_prompt_from_str(\"the big red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtoken_llm = SubtokenPotential(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'the', b' big', b' red']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>n</pre></td><td><pre>0.5599006419846053</pre></td> </tr><tr><td><pre>s</pre></td><td><pre>0.43918110663135934</pre></td> </tr><tr><td><pre>t</pre></td><td><pre>0.00043727665006992487</pre></td> </tr><tr><td><pre>f</pre></td><td><pre>0.0004310077968950805</pre></td> </tr><tr><td><pre>c</pre></td><td><pre>2.1143516265010492e-05</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{'n': 0.5599006419846053, 's': 0.43918110663135934, 't': 0.00043727665006992487, 'f': 0.0004310077968950805, 'c': 2.1143516265010492e-05}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logps = await subtoken_llm.logw_next(b\" i\", [b\" box\"])\n",
    "logps.exp().materialize(top=5).project(lambda x: bytes([x]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token sampling by sampling subtokens\n",
    "\n",
    "As a sanity check, here we demonstrate that sequentially sampling subtokens from the subtoken potential is equivalent to directly sampling tokens from the original potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sample_token(subtoken_potential, token_ctx, draw=sample_dict, g=bytes):\n",
    "    # Sample tokens by sampling subtokens until EOS is reached.\n",
    "\n",
    "    # Initialize with the weight of the empty sequence.\n",
    "    log_w = await subtoken_potential.prefix([], token_ctx)\n",
    "    log_p = 0\n",
    "    subtokens = []\n",
    "    while True:\n",
    "        # This is defined over bytes \\cup {EOS, EOT}.\n",
    "        # EOS is the end-of-sequence token (which is a subtoken).\n",
    "        # EOT is the end-of-token token.\n",
    "        logws = await subtoken_potential.logw_next(subtokens, token_ctx)\n",
    "\n",
    "        logps = logws.normalize()\n",
    "        x = draw(logps.exp())\n",
    "        log_w += logws.sum()\n",
    "        log_p += logps[x]\n",
    "\n",
    "        if x == EOS:  # Special case post-processing for EOS.\n",
    "            # Note: we could do the extra step of adding EOS to the subtoken and then immediately sampling EOT with p=1.\n",
    "            assert not subtokens, \"EOS can't come in the middle of a token.\"\n",
    "            return EOS, log_w, log_p\n",
    "\n",
    "        if x == EOT:  # We've decided to end this token.\n",
    "            return g(subtokens), log_w, log_p\n",
    "\n",
    "        subtokens.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b' guy', 2.028800662970882e-05, -3.8421235250695993)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ctx = []\n",
    "token, logw, logp = await sample_token(subtoken_llm, token_ctx)\n",
    "(token, logw, logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a probability distribution, the weights will all be 1 (modulo floating point precision). Thus, we expect the log probability of each sample of subtokens to be equal to the log probabilty of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.8421032"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logps = await llm.logw_next(token_ctx)\n",
    "logps[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is a properly weighted sampler (the following should work for non-probabilistic potentials):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from genlm_control.tracer import TraceSWOR\n",
    "from arsenal.maths import logsumexp\n",
    "\n",
    "context = []\n",
    "tracer = TraceSWOR()\n",
    "swor_logws = llm.alloc_logws()\n",
    "\n",
    "while tracer.root.mass > 0:\n",
    "    with tracer:\n",
    "        token, logw, logp = await sample_token(subtoken_llm, context, draw=tracer)\n",
    "        token_id = llm.lookup[token]\n",
    "        swor_logws[token_id] = logsumexp([swor_logws[token_id], logw + logp])\n",
    "\n",
    "have = llm.make_lazy_weights(swor_logws)\n",
    "want = await llm.logw_next(context)\n",
    "\n",
    "have.assert_equal(want, rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling arbitrary units by sampling subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bern(p):\n",
    "    return {True: p, False: 1 - p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sample_unit(\n",
    "    subtoken_potential, unit_potential, tokens, subtokens, draw=sample_dict, g=bytes\n",
    "):\n",
    "    unit = []\n",
    "    starts_unit = await unit_potential.logw_next(g(unit))\n",
    "    starts_unit = starts_unit.exp()\n",
    "\n",
    "    V = subtoken_potential.vocab[:-1]  # Excluding EOS token.\n",
    "\n",
    "    while True:\n",
    "        # Get subtoken weights.\n",
    "        u_ws = (await unit_potential.logw_next(g(unit))).exp()\n",
    "        s_ps = (await subtoken_potential.logw_next(subtokens, tokens)).normalize().exp()\n",
    "\n",
    "        # Assume, for now, that the unit potential is boolean.\n",
    "        assert np.all(np.logical_or(u_ws.weights == 0, u_ws.weights == 1))\n",
    "\n",
    "        # Maybe sample the EOS token (special case this decision).\n",
    "        # We assume that EOS is a unit.\n",
    "        eos_p = s_ps[EOS]\n",
    "        if draw(bern(eos_p)):\n",
    "            assert not subtokens, \"EOS can't come in the middle of a token.\"\n",
    "            return EOS, tokens, subtokens\n",
    "\n",
    "        # Maybe end the token.\n",
    "        p_eot = s_ps[EOT]\n",
    "        if draw(bern(p_eot)):\n",
    "            tokens.append(g(subtokens))\n",
    "            subtokens = []\n",
    "            continue\n",
    "\n",
    "        # Maybe end the unit.\n",
    "        # EOS in the unit potential is the unit boundary.\n",
    "        can_stop = u_ws[EOS]\n",
    "        if can_stop > 0:\n",
    "            p_starts = sum(s_ps[x] * starts_unit[x] for x in V)\n",
    "            p_continues = sum(s_ps[x] * u_ws[x] for x in V)\n",
    "            p_eou = p_starts / (p_starts + p_continues)\n",
    "            if draw(bern(p_eou)):\n",
    "                return g(unit), tokens, subtokens\n",
    "\n",
    "        # Sample next subtoken\n",
    "        p_next = {x: u_ws[x] * s_ps[x] for x in V}\n",
    "        subtoken = draw(p_next)\n",
    "\n",
    "        unit.append(subtoken)\n",
    "        subtokens.append(subtoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtoken_potential = SubtokenPotential(llm)\n",
    "word_potential = BoolFSA.from_regex(r\"\\s[A-Za-z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' dollar' [b' dollar'] []\n",
      "b' in' [b' dollar', b' in'] []\n",
      "b' the' [b' dollar', b' in', b' the'] []\n",
      "b' world' [b' dollar', b' in', b' the', b' world'] []\n",
      "b' and' [b' dollar', b' in', b' the', b' world', b' and'] []\n",
      "b' the' [b' dollar', b' in', b' the', b' world', b' and', b' the'] []\n",
      "b' big' [b' dollar', b' in', b' the', b' world', b' and', b' the', b' big'] []\n",
      "b' blue' [b' dollar', b' in', b' the', b' world', b' and', b' the', b' big', b' blue'] []\n",
      "b' dollar' [b' dollar', b' in', b' the', b' world', b' and', b' the', b' big', b' blue', b' dollar'] []\n",
      "b' in' [b' dollar', b' in', b' the', b' world', b' and', b' the', b' big', b' blue', b' dollar', b' in'] []\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "subtokens = []\n",
    "\n",
    "for _ in range(10):\n",
    "    unit, tokens, subtokens = await sample_unit(\n",
    "        subtoken_potential, word_potential, tokens, subtokens\n",
    "    )\n",
    "    print(unit, tokens, subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_potential = BoolFSA.from_regex(\n",
    "    r\"\\s?[A-Za-z0-9]+\\s\"\n",
    ")  # Words must end with a space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' one ' [b' one'] [32]\n",
      "b'from ' [b' one', b' from'] [32]\n",
      "b'the ' [b' one', b' from', b' the'] [32]\n",
      "b'left ' [b' one', b' from', b' the', b' left'] [32]\n",
      "b'is ' [b' one', b' from', b' the', b' left', b' is'] [32]\n",
      "b'the ' [b' one', b' from', b' the', b' left', b' is', b' the'] [32]\n",
      "b'one ' [b' one', b' from', b' the', b' left', b' is', b' the', b' one'] [32]\n",
      "b'that ' [b' one', b' from', b' the', b' left', b' is', b' the', b' one', b' that'] [32]\n",
      "b'has ' [b' one', b' from', b' the', b' left', b' is', b' the', b' one', b' that', b' has'] [32]\n",
      "b'been ' [b' one', b' from', b' the', b' left', b' is', b' the', b' one', b' that', b' has', b' been'] [32]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "subtokens = []\n",
    "# Problem: I don't think we will ever sample EOS here, because we'll almost always have token slop.\n",
    "for _ in range(10):\n",
    "    unit, tokens, subtokens = await sample_unit(\n",
    "        subtoken_potential, word_potential, tokens, subtokens\n",
    "    )\n",
    "    print(unit, tokens, subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' boxes of ' [b' boxes', b' of'] [32]\n",
      "b'the first-' [b' boxes', b' of', b' the', b' first', b'-'] []\n",
      "b'time voter' [b' boxes', b' of', b' the', b' first', b'-', b'time'] [32, 118, 111, 116, 101, 114]\n",
      "b's, with th' [b' boxes', b' of', b' the', b' first', b'-', b'time', b' voters', b',', b' with'] [32, 116, 104]\n"
     ]
    }
   ],
   "source": [
    "sentence = BoolFSA.from_regex(r\".{10}\")\n",
    "tokens = []\n",
    "subtokens = []\n",
    "\n",
    "for _ in range(4):\n",
    "    unit, tokens, subtokens = await sample_unit(\n",
    "        subtoken_potential, sentence, tokens, subtokens\n",
    "    )\n",
    "    print(unit, tokens, subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
