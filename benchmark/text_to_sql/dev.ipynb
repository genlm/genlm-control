{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from genlm_control import InferenceEngine, PromptedLLM, BoolCFG, eager_token_sampler\n",
    "\n",
    "genlm_control_path = Path.cwd().parent.parent\n",
    "if str(genlm_control_path) not in sys.path:\n",
    "    sys.path.insert(0, str(genlm_control_path))\n",
    "\n",
    "from benchmark.text_to_sql.run_inference import spider_setup  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_spider_dir = \"data/spider_data\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lm_backend = \"hf\"\n",
    "grammar_dir = \"data/grammars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data, _, prompt_formatter = spider_setup(raw_spider_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm_backend/tokenization/vocab.py:99: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = PromptedLLM.from_name(model_name, backend=lm_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = dev_data[0]\n",
    "\n",
    "llm.prompt_ids = llm.model.tokenizer.apply_chat_template(\n",
    "    prompt_formatter.format_openai(datum),\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open(os.path.join(grammar_dir, f\"{datum.schema_name}.lark\"), \"r\").read()\n",
    "bool_cfg = BoolCFG.from_lark(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/genlm/lib/python3.11/site-packages/genlm_backend/trie/parallel.py:63: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  ).to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "sampler = eager_token_sampler(llm, bool_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genlm_control.experimental.vegas import GumbelMaxAdaptiveRejectionSampler\n",
    "\n",
    "sampler = GumbelMaxAdaptiveRejectionSampler(llm, bool_cfg.coerce(llm, f=b\"\".join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = dev_data[0]\n",
    "\n",
    "llm.prompt_ids = llm.model.tokenizer.apply_chat_template(\n",
    "    prompt_formatter.format_openai(datum),\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m]\u001b[0m\n",
      "-0.00:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m|\u001b[0mEOS\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m|\u001b[0mEOS\u001b[0;35m]\u001b[0m\n",
      "-0.01:\t\u001b[0;35m[\u001b[0mb'SELECT'\u001b[0;35m|\u001b[0mb' COUNT'\u001b[0;35m|\u001b[0mb'(S'\u001b[0;35m|\u001b[0mb'inger'\u001b[0;35m|\u001b[0mb'_ID'\u001b[0;35m|\u001b[0mb')'\u001b[0;35m|\u001b[0mb' FROM'\u001b[0;35m|\u001b[0mb' singer'\u001b[0;35m|\u001b[0mb';'\u001b[0;35m|\u001b[0mEOS\u001b[0;35m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sequences = await InferenceEngine(sampler)(\n",
    "    n_particles=3,\n",
    "    max_tokens=100,\n",
    "    ess_threshold=0.9,\n",
    "    verbosity=1,\n",
    "    # json_path=os.path.join('results/test', f'0.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
