{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import kl_divergence_potentials, kl_divergence_sequences\n",
    "from genlm.control import PromptedLLM, direct_token_sampler, AWRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahya/mambaforge/envs/genlm/lib/python3.12/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling completions from creative model...\n"
     ]
    }
   ],
   "source": [
    "model_p = PromptedLLM.from_name(\n",
    "    \"gpt2\", backend=\"hf\", temperature=1.2, eos_tokens=[b\".\"]\n",
    ")\n",
    "model_p.set_prompt_from_str(\"Once upon a time, in a magical forest,\")\n",
    "\n",
    "# Model Q: Factual prompt with lower temperature\n",
    "model_q = model_p.spawn()\n",
    "model_q.set_prompt_from_str(\"The capital of France is\")\n",
    "model_q.temperature = 0.8\n",
    "\n",
    "# Actually sample completions from model P using SMC\n",
    "print(\"Sampling completions from creative model...\")\n",
    "sampler_p = direct_token_sampler(model_p)\n",
    "\n",
    "sequences_p = await sampler_p.smc(n_particles=10, max_tokens=15, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahya/mambaforge/envs/genlm/lib/python3.12/site-packages/IPython/extensions/deduperreload/deduperreload.py:290: DeprecationWarning: ast.Ellipsis is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  elif not isinstance(ast_elt, (ast.Ellipsis, ast.Pass)):\n"
     ]
    }
   ],
   "source": [
    "samples = list(sequences_p.decoded_posterior.keys())\n",
    "kl_div = await kl_divergence_potentials(model_p, model_q, samples)\n",
    "# samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b' you', b' walked', b' into', b' a', b' blue', b' dancing', b' statue']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p.tokenize(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KL(Creative||Factual) = 27.9045\n",
      "\n",
      "Individual log probabilities:\n",
      "  ' you walked into a blue dancing statue':\n",
      "    Creative model: -37.104\n",
      "    Factual model:  -59.622\n",
      "    Log ratio:      22.518\n",
      "  ' thousands count as hourly':\n",
      "    Creative model: -33.983\n",
      "    Factual model:  -54.983\n",
      "    Log ratio:      21.000\n",
      "\n",
      "üéØ Key insight: We sampled actual completions from model P,\n",
      "   then evaluated both P(x) and Q(x) on those samples!\n",
      "   This gives us KL(P||Q) = E_P[log P(x) - log Q(x)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nKL(Creative||Factual) = {kl_div:.4f}\")\n",
    "\n",
    "# Also demonstrate individual log probabilities for first few samples\n",
    "print(\"\\nIndividual log probabilities:\")\n",
    "for sample in samples[:2]:  # Just show first 2\n",
    "    tokens = model_p.tokenize(sample)\n",
    "    logp_p = await model_p.complete(tokens)\n",
    "    logp_q = await model_q.complete(tokens)\n",
    "    print(f\"  '{sample}':\")\n",
    "    print(f\"    Creative model: {logp_p:.3f}\")\n",
    "    print(f\"    Factual model:  {logp_q:.3f}\")\n",
    "    print(f\"    Log ratio:      {logp_p - logp_q:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from constrained model (with FSA)...\n",
      "Sampling from unconstrained model...\n",
      "\n",
      "Constrained samples (1):\n",
      "  1. ' SMC is üî•üî• with LMs'\n",
      "\n",
      "Unconstrained samples (5):\n",
      "  1. ' Under TNA's contract and MRG's Agreement with WWE, puts members of my community in direct competition with potential NXT titles.' (length: 129)\n",
      "  2. ' It's no secret (or recommended, at least) that she is HIV negative.' (length: 68)\n",
      "  3. ' what motivates me, is the question Europe has been asking ourselves. It is time for a national debate.' (length: 103)\n",
      "\n",
      "DEBUG: Analyzing constrained sample...\n",
      "Sample: ' SMC is üî•üî• with LMs'\n",
      "Tokens: [b' SM', b'C', b' is', b' \\xf0\\x9f', b'\\x94', b'\\xa5', b'\\xf0\\x9f', b'\\x94', b'\\xa5', b' with', b' L', b'Ms']\n",
      "Constrained log prob: -60.691611766815186\n",
      "Unconstrained log prob: -60.691611766815186\n",
      "Log ratio: 0.0\n",
      "^ This is 0 because both models are the same underlying LLM!\n",
      "\n",
      "üí° Key insight: The FSA constraint affects sampling behavior,\n",
      "   but both models use the same LLM for probability evaluation!\n",
      "   So KL divergence between the base models is 0.\n",
      "\n",
      "Valid unconstrained samples: 5 out of 5\n",
      "\n",
      "Trying KL with valid samples...\n",
      "KL(Unconstrained || Constrained) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from genlm.control import PromptedLLM, BoolFSA, direct_token_sampler\n",
    "from metrics import kl_divergence_potentials\n",
    "\n",
    "\n",
    "async def compare_constrained_vs_unconstrained():\n",
    "    \"\"\"Compare KL divergence between constrained and unconstrained models.\"\"\"\n",
    "\n",
    "    # Model P: Constrained model (with FSA constraint)\n",
    "    llm_constrained = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\"])\n",
    "    llm_constrained.set_prompt_from_str(\"Here is my honest opinion:\")\n",
    "\n",
    "    # Create FSA constraint\n",
    "    fsa = BoolFSA.from_regex(r\" SMC is (üî•üî•|üòçüòç|ü§åü§å) with LMs\")\n",
    "    coerced_fsa = fsa.coerce(llm_constrained, f=b\"\".join)\n",
    "\n",
    "    # Constrained sampler\n",
    "    constrained_sampler = AWRS(llm_constrained, coerced_fsa)\n",
    "\n",
    "    # Model Q: Unconstrained model (same LLM, no FSA)\n",
    "    llm_unconstrained = llm_constrained.spawn()  # Same prompt, no constraints\n",
    "    llm_unconstrained.set_prompt_from_str(\"Here is my honest opinion:\")\n",
    "    unconstrained_sampler = direct_token_sampler(llm_unconstrained)\n",
    "\n",
    "    print(\"Sampling from constrained model (with FSA)...\")\n",
    "    constrained_sequences = await constrained_sampler.smc(\n",
    "        n_particles=10, ess_threshold=0.5, max_tokens=30, verbosity=0\n",
    "    )\n",
    "\n",
    "    print(\"Sampling from unconstrained model...\")\n",
    "    unconstrained_sequences = await unconstrained_sampler.smc(\n",
    "        n_particles=10, ess_threshold=0.5, max_tokens=30, verbosity=0\n",
    "    )\n",
    "\n",
    "    # Get samples from constrained model\n",
    "    constrained_samples = list(constrained_sequences.decoded_posterior.keys())\n",
    "    unconstrained_samples = list(unconstrained_sequences.decoded_posterior.keys())\n",
    "\n",
    "    print(f\"\\nConstrained samples ({len(constrained_samples)}):\")\n",
    "    for i, sample in enumerate(constrained_samples[:3]):\n",
    "        print(f\"  {i + 1}. '{sample}'\")\n",
    "\n",
    "    print(f\"\\nUnconstrained samples ({len(unconstrained_samples)}):\")\n",
    "    for i, sample in enumerate(unconstrained_samples[:3]):\n",
    "        print(f\"  {i + 1}. '{sample}' (length: {len(sample)})\")\n",
    "\n",
    "    if constrained_samples:\n",
    "        # Debug: Show why KL might be 0\n",
    "        print(\"\\nDEBUG: Analyzing constrained sample...\")\n",
    "        sample = constrained_samples[0]\n",
    "        tokens = llm_constrained.tokenize(sample)\n",
    "        print(f\"Sample: '{sample}'\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "\n",
    "        # The issue: constrained model vs unconstrained model are the SAME model!\n",
    "        # They share the same underlying LLM, so they give identical probabilities\n",
    "        logp_constrained = await llm_constrained.complete(tokens)\n",
    "        logp_unconstrained = await llm_unconstrained.complete(tokens)\n",
    "        print(f\"Constrained log prob: {logp_constrained}\")\n",
    "        print(f\"Unconstrained log prob: {logp_unconstrained}\")\n",
    "        print(f\"Log ratio: {logp_constrained - logp_unconstrained}\")\n",
    "        print(\"^ This is 0 because both models are the same underlying LLM!\")\n",
    "\n",
    "        # The constraint only affects SAMPLING, not the probability evaluation\n",
    "        print(\"\\nüí° Key insight: The FSA constraint affects sampling behavior,\")\n",
    "        print(\"   but both models use the same LLM for probability evaluation!\")\n",
    "        print(\"   So KL divergence between the base models is 0.\")\n",
    "\n",
    "    # Filter out empty samples\n",
    "    valid_unconstrained_samples = [s for s in unconstrained_samples if s and s.strip()]\n",
    "    print(\n",
    "        f\"\\nValid unconstrained samples: {len(valid_unconstrained_samples)} out of {len(unconstrained_samples)}\"\n",
    "    )\n",
    "\n",
    "    if valid_unconstrained_samples:\n",
    "        print(\"\\nTrying KL with valid samples...\")\n",
    "        try:\n",
    "            kl_u_to_c = await kl_divergence_potentials(\n",
    "                llm_unconstrained, llm_constrained, valid_unconstrained_samples\n",
    "            )\n",
    "            print(f\"KL(Unconstrained || Constrained) = {kl_u_to_c:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    # Cleanup\n",
    "    await constrained_sampler.cleanup()\n",
    "    await unconstrained_sampler.cleanup()\n",
    "\n",
    "\n",
    "# Run it\n",
    "await compare_constrained_vs_unconstrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahya/mambaforge/envs/genlm/lib/python3.12/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n",
      "/Users/yahya/Desktop/genlm/genlm-control/genlm/control/potential/product.py:68: RuntimeWarning: Common vocabulary (227 tokens) is less than 10.0% of p1's (PromptedLLM(prompt=[b'Here', b' is', b' my', b' honest', b' opinion', b':'])) vocabulary (50256 tokens). This Product potential only operates on this relatively small subset of tokens.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(Constrained_samples || Unconstrained_samples) = 23.0259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(23.025850929940457)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def compare_sampling_distributions():\n",
    "    \"\"\"Compare the actual sampling distributions, not the base models.\"\"\"\n",
    "\n",
    "    # Same base model, different constraints\n",
    "    llm = PromptedLLM.from_name(\"gpt2\", eos_tokens=[b\"\\n\"])\n",
    "    llm.set_prompt_from_str(\"Here is my honest opinion:\")\n",
    "\n",
    "    # Constrained sampler\n",
    "    fsa = BoolFSA.from_regex(r\" SMC is (üî•üî•|üòçüòç|ü§åü§å) with LMs\")\n",
    "    coerced_fsa = fsa.coerce(llm, f=b\"\".join)\n",
    "    constrained_sampler = AWRS(llm, coerced_fsa)\n",
    "\n",
    "    # Unconstrained sampler\n",
    "    unconstrained_sampler = direct_token_sampler(llm)\n",
    "\n",
    "    # Sample from both\n",
    "    constrained_sequences = await constrained_sampler.smc(\n",
    "        n_particles=20, max_tokens=30, ess_threshold=0.5\n",
    "    )\n",
    "    unconstrained_sequences = await unconstrained_sampler.smc(\n",
    "        n_particles=20, max_tokens=30, ess_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # Use kl_divergence_sequences to compare the sampling distributions directly!\n",
    "    kl_div = kl_divergence_sequences(constrained_sequences, unconstrained_sequences)\n",
    "    print(f\"KL(Constrained_samples || Unconstrained_samples) = {kl_div:.4f}\")\n",
    "\n",
    "    # This measures how different the actual sampling distributions are\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "await compare_sampling_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
